{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试在数据集上的实际准确率\n",
    "    - 读取视频\n",
    "    - 提取骨架\n",
    "    - 特征增强\n",
    "    - 预测，计算准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pelicuals = r'C:\\Users\\weightwater\\Desktop\\final\\dataset\\Peliculas\\Peliculas\\fights\\newfi87.avi'\n",
    "rwf2000 = r'C:\\Users\\weightwater\\Desktop\\final\\dataset\\RWF2000\\RWF-2000\\RWF-2000\\train\\Fight\\6mrIIcAQ2fI_3.avi'\n",
    "fdsd = r'C:\\Users\\weightwater\\Desktop\\final\\dataset\\fight-detection-surv-dataset-master\\fight\\fi071.mp4'\n",
    "vvv = r'C:\\Users\\weightwater\\Desktop\\final\\dataset\\data\\Videos\\Violent_00503.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vedio = vvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 读取视频\n",
    "    - 输入视频地址，并配置是否使用关键帧和光流筛选，默认不使用\n",
    "    - 返回视频序列，类型为ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vedio(vedioPath, useKeyFrame=False, useOptical=False, cropScale=0.8):\n",
    "    cap = cv.VideoCapture(vedioPath)\n",
    "    num_frame = int(cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "    height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "    channel = 3\n",
    "\n",
    "    # 读取视频中的所有帧，用作之后计算\n",
    "    frameSeq = np.zeros((num_frame, height, width, channel), dtype=np.uint8)\n",
    "    for i in range(num_frame):\n",
    "        ret, frame = cap.read()\n",
    "        frameSeq[i] = frame\n",
    "        # cv2.imshow('test', frame[top:bottom, left:right, :])\n",
    "        # cv2.waitKey(30)\n",
    "\n",
    "    # 获取要选择的帧\n",
    "    index = list(range(num_frame))\n",
    "    if useKeyFrame:\n",
    "        index = getKeyFrame(frameSeq, window=10)\n",
    "\n",
    "    # 获取要截选的区域\n",
    "    left, right = 0, width\n",
    "    top, bottom = 0, height\n",
    "    if useOptical:\n",
    "        top, bottom, left, right = opticalCalculate(frameSeq, cropScale=cropScale)\n",
    "\n",
    "    # 读取图片返回图片序列\n",
    "    # opencv 图片存储数据为uint8\n",
    "    newFrameSeq = np.zeros((len(index), bottom-top, right-left, channel), dtype=np.uint8)\n",
    "    # print(newFrameSeq.shape, frameSeq.shape, top, bottom, left, right)\n",
    "    num = 0\n",
    "    for i in range(num_frame):\n",
    "        if i in index:\n",
    "            newFrameSeq[num] = frameSeq[i, top:bottom, left:right, :]\n",
    "            num += 1\n",
    "        # cv.imshow('test', frameSeq[i, top:bottom, left:right, :])\n",
    "        # cv.waitKey(30)\n",
    "\n",
    "    return newFrameSeq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n"
     ]
    }
   ],
   "source": [
    "test_read = test_vedio\n",
    "frameSeq = read_vedio(test_read)\n",
    "print(frameSeq.dtype)\n",
    "# for frame in res:\n",
    "#     cv2.imshow('test', frame)\n",
    "#     cv2.waitKey(50)\n",
    "# cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提取骨架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_dir = 'C:\\\\Users\\\\weightwater\\\\Desktop\\\\final\\\\openpose\\\\openpose\\\\openpose-1.7.0\\\\examples\\\\media\\\\COCO_val2014_000000000192.jpg'\n",
    "model_dir = 'C:\\\\Users\\\\weightwater\\\\Desktop\\\\final\\\\openpose\\\\openpose\\\\openpose-1.7.0\\\\models'\n",
    "sys_dir = 'C:\\\\Users\\\\weightwater\\\\Desktop\\\\final\\\\openpose\\\\openpose\\\\openpose-1.7.0\\\\build\\\\python\\\\openpose\\\\Debug'\n",
    "os_dir = ';' + 'C:\\\\Users\\\\weightwater\\\\Desktop\\\\final\\\\openpose\\\\openpose\\\\openpose-1.7.0\\\\build\\\\x64\\\\Debug;' + 'C:\\\\Users\\\\weightwater\\\\Desktop\\\\final\\\\openpose\\\\openpose\\\\openpose-1.7.0\\\\build\\\\bin'\n",
    "\n",
    "try:\n",
    "    # Change these variables to point to the correct folder (Release/x64 etc.)\n",
    "    sys.path.append(sys_dir)\n",
    "    os.environ['PATH']  = os.environ['PATH'] + os_dir\n",
    "    import pyopenpose as op\n",
    "    params = dict()\n",
    "    params[\"model_folder\"] = model_dir\n",
    "    params[\"net_resolution\"] = '160x80'\n",
    "    params['num_gpu'] = 1\n",
    "except ImportError as e:\n",
    "    print('Error: OpenPose library could not be found. Did you enable `BUILD_PYTHON` in CMake and have this Python script in the right folder?')\n",
    "    print(e)\n",
    "    sys.exit(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opWrapper = op.WrapperPython()\n",
    "opWrapper.configure(params)\n",
    "opWrapper.start()\n",
    "\n",
    "# Process Image\n",
    "datum = op.Datum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poseFromVedio(frameSeq, datum):\n",
    "    poseSeq = []\n",
    "    for frame in frameSeq:\n",
    "        \n",
    "        datum.cvInputData = frame\n",
    "        opWrapper.emplaceAndPop(op.VectorDatum([datum]))\n",
    "        data = datum.poseKeypoints\n",
    "\n",
    "        if data is not None:\n",
    "            poseSeq.append(data.tolist())\n",
    "    return poseSeq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "poseSeq = poseFromVedio(frameSeq, datum)\n",
    "print(len(frameSeq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.read()\n",
    "    data = json.loads(data)\n",
    "    res = []\n",
    "    for item in data['people']:\n",
    "        tmp = [item['pose_keypoints_2d'][i*3: i*3+3] for i in range(len(item['pose_keypoints_2d']) // 3)]\n",
    "        \n",
    "        counts0 = sum(1 if p == 0 else 0 for p in [item['pose_keypoints_2d']][0])\n",
    "        if counts0 > 10:\n",
    "            continue\n",
    "        res.append(tmp)\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def pointDistance(keyPoint):\n",
    "    \"\"\"\n",
    "    :param keyPoint:\n",
    "    :return:list\n",
    "    :distance:\n",
    "    \"\"\"\n",
    "    distance0 = (keyPoint[4][0] - keyPoint[9][0]) ** 2 + (keyPoint[4][1] - keyPoint[9][1]) ** 2\n",
    "    distance1 = (keyPoint[7][0] - keyPoint[12][0]) ** 2 + (keyPoint[7][1] - keyPoint[12][1]) ** 2\n",
    "    distance2 = (keyPoint[2][0] - keyPoint[4][0]) ** 2 + (keyPoint[2][1] - keyPoint[4][1]) ** 2\n",
    "    distance3 = (keyPoint[5][0] - keyPoint[7][0]) ** 2 + (keyPoint[5][1] - keyPoint[7][1]) ** 2\n",
    "    distance4 = (keyPoint[0][0] - keyPoint[4][0]) ** 2 + (keyPoint[0][1] - keyPoint[4][1]) ** 2\n",
    "    distance5 = (keyPoint[0][0] - keyPoint[7][0]) ** 2 + (keyPoint[0][1] - keyPoint[7][1]) ** 2\n",
    "    distance6 = (keyPoint[4][0] - keyPoint[10][0]) ** 2 + (keyPoint[4][1] - keyPoint[10][1]) ** 2\n",
    "    distance7 = (keyPoint[7][0] - keyPoint[13][0]) ** 2 + (keyPoint[7][1] - keyPoint[13][1]) ** 2\n",
    "    distance8 = (keyPoint[4][0] - keyPoint[7][0]) ** 2 + (keyPoint[4][1] - keyPoint[7][1]) ** 2\n",
    "    distance9 = (keyPoint[11][0] - keyPoint[14][0]) ** 2 + (keyPoint[11][1] - keyPoint[14][1]) ** 2\n",
    "    distance10 = (keyPoint[10][0] - keyPoint[13][0]) ** 2 + (keyPoint[10][1] - keyPoint[13][1]) ** 2\n",
    "    distance11 = (keyPoint[6][0] - keyPoint[10][0]) ** 2 + (keyPoint[6][1] - keyPoint[10][1]) ** 2\n",
    "    distance12 = (keyPoint[3][0] - keyPoint[13][0]) ** 2 + (keyPoint[3][1] - keyPoint[13][1]) ** 2\n",
    "    distance13 = (keyPoint[4][0] - keyPoint[23][0]) ** 2 + (keyPoint[4][1] - keyPoint[23][1]) ** 2\n",
    "    distance14 = (keyPoint[7][0] - keyPoint[20][0]) ** 2 + (keyPoint[7][1] - keyPoint[20][1]) ** 2\n",
    "\n",
    "    return [distance0, distance1, distance2, distance3, distance4, distance5, distance6, distance7,\n",
    "            distance8, distance9, distance10, distance11, distance12, distance13, distance14]\n",
    "\n",
    "\n",
    "def pointAngle(keyPoint):\n",
    "    angle0 = __myAngle(keyPoint[2], keyPoint[3], keyPoint[4])\n",
    "    angle1 = __myAngle(keyPoint[5], keyPoint[6], keyPoint[7])\n",
    "    angle2 = __myAngle(keyPoint[9], keyPoint[10], keyPoint[11])\n",
    "    angle3 = __myAngle(keyPoint[12], keyPoint[13], keyPoint[14])\n",
    "    angle4 = __myAngle(keyPoint[3], keyPoint[2], keyPoint[1])\n",
    "    angle5 = __myAngle(keyPoint[6], keyPoint[5], keyPoint[1])\n",
    "    angle6 = __myAngle(keyPoint[10], keyPoint[8], keyPoint[13])\n",
    "    angle7 = __myAngle(keyPoint[7], keyPoint[12], keyPoint[13])\n",
    "    angle8 = __myAngle(keyPoint[4], keyPoint[9], keyPoint[10])\n",
    "    angle9 = __myAngle(keyPoint[4], keyPoint[0], keyPoint[7])\n",
    "    angle10 = __myAngle(keyPoint[4], keyPoint[8], keyPoint[7])\n",
    "    angle11 = __myAngle(keyPoint[1], keyPoint[8], keyPoint[13])\n",
    "    angle12 = __myAngle(keyPoint[1], keyPoint[8], keyPoint[10])\n",
    "    angle13 = __myAngle(keyPoint[4], keyPoint[1], keyPoint[8])\n",
    "    angle14 = __myAngle(keyPoint[7], keyPoint[1], keyPoint[8])\n",
    "\n",
    "    return [angle0, angle1, angle2, angle3, angle4, angle5, angle6, angle7,\n",
    "            angle8, angle9, angle10, angle11, angle12, angle13, angle14]\n",
    "\n",
    "\n",
    "def __myAngle(A, B, C):\n",
    "    c = math.sqrt((A[0] - B[0]) ** 2 + (A[1] - B[1]) ** 2)\n",
    "    a = math.sqrt((B[0] - C[0]) ** 2 + (B[1] - C[1]) ** 2)\n",
    "    b = math.sqrt((A[0] - C[0]) ** 2 + (A[1] - C[1]) ** 2)\n",
    "    if 2 * a * c != 0:\n",
    "        return (a ** 2 + c ** 2 - b ** 2) / (2 * a * c)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_dis_ang(data):\n",
    "    # data = read_json(json_path)\n",
    "    res = []\n",
    "    for people in data:\n",
    "        distance = pointDistance(people)\n",
    "        angle = pointAngle(people)\n",
    "        res.append((distance, angle))\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def standardization(data):\n",
    "    mu = np.mean(data, axis=0)\n",
    "    sigma = np.std(data, axis=0)\n",
    "    return (data - mu) / sigma\n",
    "\n",
    "# input need pose list contained pose\n",
    "def get_x(data):\n",
    "    # dis_ang = get_dis_ang(json_path)\n",
    "    dis_ang = get_dis_ang(data)\n",
    "    t = []\n",
    "    for dis, ang in dis_ang:\n",
    "        st_dis = standardization(np.array(dis))\n",
    "        \n",
    "        t_dis = torch.from_numpy(st_dis)\n",
    "        t_ang = torch.tensor(st_dis)\n",
    "        tmp = torch.cat((t_dis, t_ang))\n",
    "        tmp = tmp.view(len(tmp), 1)\n",
    "        t.append(tmp)\n",
    "        \n",
    "        \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4], [4, 5]]\n"
     ]
    }
   ],
   "source": [
    "a = [[1, 2], [3, 4]]\n",
    "b = [[4, 5]]\n",
    "c = a+b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose2Feature(poseSeq):\n",
    "    featureSeq = []\n",
    "    for pose in poseSeq:\n",
    "        tmp = get_x(pose)\n",
    "        featureSeq.append(tmp)\n",
    "    return featureSeq\n",
    "\n",
    "# featureSeq = pose2Feature(poseSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(featureSeq[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检测视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViolentClassification(nn.Module):\n",
    "    # 30 200 300 100 2\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, n_hidden_3, out_dim):\n",
    "        super(ViolentClassification, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, n_hidden_1)\n",
    "        self.fc2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.fc3 = nn.Linear(n_hidden_2, n_hidden_3)\n",
    "        self.fc4 = nn.Linear(n_hidden_3, out_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = self.relu(self.fc1(data))\n",
    "        data = self.relu(self.fc2(data))\n",
    "        data = self.relu(self.fc3(data))\n",
    "        output = self.fc4(data)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ViolentClassification(30, 200, 300, 100, 2)\n",
    "net.load_state_dict(torch.load('./model/version1.pth'))\n",
    "\n",
    "def checkViolent(featureSeq, threshold=0.7):\n",
    "    num_Violent = 0\n",
    "    for featrues in featureSeq:\n",
    "        violent = False\n",
    "        for f in featrues:\n",
    "            # print(f)\n",
    "            output = net(torch.tensor(f).view(1, 30).float())\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            # print(predicted[0])\n",
    "            if predicted[0] == 1:\n",
    "                violent = True\n",
    "                break\n",
    "        if violent:\n",
    "            num_Violent += 1\n",
    "    \n",
    "    # print(num_Violent)\n",
    "\n",
    "    return num_Violent > len(featureSeq) * threshold\n",
    "\n",
    "        \n",
    "# checkViolent(featureSeq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检测一条龙，读取视频，提取骨架，特征增强，行为检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long(vedioPath, datum, useKeyFrame=False, useOptical=False):\n",
    "    tic = time.time()\n",
    "    frameSeq = read_vedio(vedioPath, useKeyFrame=useKeyFrame, useOptical=useOptical)\n",
    "    poseSeq = poseFromVedio(frameSeq, datum)\n",
    "    featureSeq = pose2Feature(poseSeq)\n",
    "    res = checkViolent(featureSeq)\n",
    "    toc = time.time()\n",
    "    return res, toc-tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weightwater\\Desktop\\final\\dataset\\data\\Videos\\Violent_00503.mp4\n",
      "True 7.595971345901489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\weightwater\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "res, t = long(test_vedio, datum)\n",
    "print(test_vedio)\n",
    "print(res, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试准确率和不使用光流和关键帧所消耗的时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    " \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noneTime, right = 0.0, 0\n",
    "testPath = r'C:\\Users\\weightwater\\Desktop\\final\\dataset\\data\\timeVedio'\n",
    "\n",
    "for i, videoName in enumerate(os.listdir(testPath)):\n",
    "    violent = True\n",
    "    if videoName[0] == 'N':\n",
    "        violent = False\n",
    "    res, t = long(testPath + '/' + videoName, datum=datum)\n",
    "    if res == violent:\n",
    "        right += 1\n",
    "    noneTime += t\n",
    "    print('[%d] vedio cost time: %.3f result: ' % (i, t) ,str(res==violent))\n",
    "\n",
    "accuracy = right / len(os.listdir(testPath))\n",
    "print('cost time: ', noneTime, 'accuracy: ', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "print(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加入光流\n",
    "    - 统计一下视频的尺寸，设置光流参数\n",
    "    - 整合光流\n",
    "      - 输入视频，返回选取的视频的边界\n",
    "    - 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opticalCalculate(frameSeq, cropScale=0.9, resize=(224, 224)):\n",
    "    num_frame, hight, width, channel = frameSeq.shape\n",
    "    opt_set = np.zeros((num_frame-1, resize[0], resize[1], 2))\n",
    "    frame1 = frameSeq[0]\n",
    "    frame1 = cv.resize(frame1, resize, interpolation=cv.INTER_AREA)\n",
    "    prvs = cv.cvtColor(frame1, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    for i, frame2 in enumerate(frameSeq[1:]):\n",
    "        frame2 = cv.resize(frame2, resize, interpolation=cv.INTER_AREA)\n",
    "        next = cv.cvtColor(frame2, cv.COLOR_BGR2GRAY)\n",
    "        flow = cv.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "        prvs = next\n",
    "        opt_set[i] = flow\n",
    "    \n",
    "    crop_size = (int(resize[0]*cropScale), int(resize[1]*cropScale))\n",
    "    optSum = np.sum(np.sqrt(opt_set[..., 0]**2 + opt_set[..., 1]**2), axis=0)\n",
    "\n",
    "    # filter slight noise by threshold\n",
    "    thresh = np.mean(optSum)\n",
    "    optSum[optSum < thresh] = 0\n",
    "\n",
    "    # calculate the center of gravity of magnitude map and adding 0.001 to avoid empty value\n",
    "    x_sum = np.sum(optSum, axis=0) + 0.001\n",
    "    y_sum = np.sum(optSum, axis=1) + 0.001\n",
    "\n",
    "    # calculate prob in every row and column\n",
    "    x_prob = x_sum / np.sum(x_sum)\n",
    "    y_prob = y_sum / np.sum(y_sum)\n",
    "\n",
    "    # print(x_prob.shape)\n",
    "\n",
    "    x, y = 0, 0\n",
    "    for index, (i, j) in enumerate(zip(x_prob, y_prob)):\n",
    "        x += index*i\n",
    "        y += index*j\n",
    "\n",
    "    x, y = int(x), int(y)\n",
    "\n",
    "    # avoid to beyond boundaries\n",
    "    nug_x = crop_size[0] // 2\n",
    "    nug_y = crop_size[1] // 2\n",
    "    x = max(nug_x, min(x, resize[0]-nug_x))\n",
    "    y = max(nug_y, min(y, resize[1]-nug_y))\n",
    "\n",
    "    left_scale, right_scale, top_scale, bottom_scale = (y-nug_y) / resize[0], (y+nug_y) / resize[0], (x-nug_x) / resize[1], (x+nug_x) / resize[1]\n",
    "\n",
    "    left, right = int(width*left_scale), int(width*right_scale)\n",
    "    top, bottom = int(hight*top_scale), int(hight*bottom_scale)\n",
    "\n",
    "    # print(left_scale, right_scale, top_scale, bottom_scale, x, y, nug_x, nug_y)\n",
    "\n",
    "    return top, bottom, left, right\n",
    "\n",
    "    # optSum = cv.normalize(optSum[x-nug_x: x+nug_x, y-nug_y: y+nug_y], None, 0, 255, cv.NORM_MINMAX).astype(np.int8)\n",
    "    # cv.imshow('optsum', optSum)\n",
    "    # cv.waitKey(0)\n",
    "    # cv.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09375 0.8883928571428571 0.004464285714285714 0.7991071428571429 90 110 89 89\n"
     ]
    }
   ],
   "source": [
    "top, bottom, left, right = opticalCalculate(frameSeq, cropScale=0.8)\n",
    "for frame in frameSeq:\n",
    "    cv.imshow('test', frame[top: bottom, left: right, :])\n",
    "    cv.waitKey(30)\n",
    "\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = read_vedio(test_vedio, useOptical=True)\n",
    "res0 = read_vedio(test_vedio, useOptical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107, 472, 840, 3) (107, 375, 668, 3)\n"
     ]
    }
   ],
   "source": [
    "print(res0.shape, res1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "res0 = long(test_vedio, datum, useOptical=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = long(test_vedio, datum, useOptical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 7.447792053222656) (True, 8.486770629882812)\n"
     ]
    }
   ],
   "source": [
    "print(res0, res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noneTime, right = 0.0, 0\n",
    "testPath = r'C:\\Users\\weightwater\\Desktop\\final\\dataset\\data\\timeVedio'\n",
    "\n",
    "for i, videoName in enumerate(os.listdir(testPath)):\n",
    "    violent = True\n",
    "    if videoName[0] == 'N':\n",
    "        violent = False\n",
    "    res, t = long(testPath + '/' + videoName, datum=datum, useOptical=True)\n",
    "    if res == violent:\n",
    "        right += 1\n",
    "    noneTime += t\n",
    "    print('[%d] vedio cost time: %.3f result: ' % (i, t) ,str(res==violent))\n",
    "\n",
    "accuracy = right / len(os.listdir(testPath))\n",
    "print('cost time: ', noneTime, 'accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加入帧差分关键帧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precess_image(image):\n",
    "    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    gray_image = cv.GaussianBlur(gray_image, (3, 3), 0)\n",
    "    return gray_image\n",
    "\n",
    "\n",
    "def abs_diff(pre_image, curr_image):\n",
    "    gray_pre = precess_image(pre_image)\n",
    "    gray_curr = precess_image(curr_image)\n",
    "    diff = cv.absdiff(gray_pre, gray_curr)\n",
    "    res, diff = cv.threshold(diff, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "    cnt_diff = np.sum(diff)\n",
    "    return cnt_diff\n",
    "\n",
    "\n",
    "def exponential_smoothing(alpha, s):\n",
    "    s_temp = [s[0]]\n",
    "    # print(s_temp)\n",
    "    for i in range(1, len(s)):\n",
    "        s_temp.append(alpha * s[i-1] + (1-alpha) * s_temp[i-1])\n",
    "    return s_temp\n",
    "\n",
    "\n",
    "def getKeyFrame(frameSeq, window=25, alpha=0.07, smooth=True):\n",
    "    num_frame = len(frameSeq)\n",
    "    index = []\n",
    "    diff = []\n",
    "    frm = 0\n",
    "    pre_image = np.array([])\n",
    "    curr_image = np.array([])\n",
    "\n",
    "    pre_image = frameSeq[0]\n",
    "\n",
    "    for i, frame in enumerate(frameSeq, 1):\n",
    "        curr_image = frame\n",
    "        diff.append(abs_diff(pre_image, curr_image))\n",
    "        pre_image = curr_image\n",
    "    \n",
    "    if smooth:\n",
    "        diff = exponential_smoothing(alpha, diff)\n",
    "    \n",
    "    diff = np.array(diff)\n",
    "    mean = np.mean(diff)\n",
    "    dev = np.std(diff)\n",
    "    diff = (diff - mean) / dev\n",
    "\n",
    "    # print('pick index')\n",
    "    for i, d in enumerate(diff):\n",
    "        ub = len(diff) - 1\n",
    "        lb = 0\n",
    "        if not i-window // 2 < lb:\n",
    "            lb = i - window//2\n",
    "        if not i-window // 2 > ub:\n",
    "            ub = i + window//2\n",
    "\n",
    "        comp_window = diff[lb:ub]\n",
    "        if d >= max(comp_window):\n",
    "            index.append(i)\n",
    "\n",
    "    tmp = np.array(index)\n",
    "    tmp = tmp + 1\n",
    "    index = tmp.tolist()\n",
    "    # print(\"Extract the Frame Index:\" + str(index))\n",
    "    return index\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 35, 49, 60, 78, 106]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getKeyFrame(frameSeq, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noneTime, right = 0.0, 0\n",
    "testPath = r'C:\\Users\\weightwater\\Desktop\\final\\dataset\\data\\timeVedio'\n",
    "\n",
    "for i, videoName in enumerate(os.listdir(testPath)):\n",
    "    violent = True\n",
    "    if videoName[0] == 'N':\n",
    "        violent = False\n",
    "    res, t = long(testPath + '/' + videoName, datum=datum, useKeyFrame=True)\n",
    "    if res == violent:\n",
    "        right += 1\n",
    "    noneTime += t\n",
    "    print('[%d] vedio cost time: %.3f result: ' % (i, t) ,str(res==violent))\n",
    "\n",
    "accuracy = right / len(os.listdir(testPath))\n",
    "print('cost time: ', noneTime, 'accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "testVedio = r'C:\\Users\\weightwater\\Desktop\\final\\大创文件\\ffffff\\fight1.mp4'\n",
    "\n",
    "res, t = long(testVedio, datum=datum, useKeyFrame=True, useOptical=True)\n",
    "# res, t = long(test_vedio, datum=datum, useKeyFrame=True, useOptical=True)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 320 568 3\n"
     ]
    }
   ],
   "source": [
    "cap = cv.VideoCapture(r'C:\\Users\\weightwater\\Desktop\\final\\大创文件\\ffffff\\fight1.mp4')\n",
    "num_frame = int(cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "channel = 3\n",
    "print(num_frame, height, width, channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1215c2695fb854c4ba7da04e87047ff6ad0cd3a897d5f699d6446d61aa011c7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
